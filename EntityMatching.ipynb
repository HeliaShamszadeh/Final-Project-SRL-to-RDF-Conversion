{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Downgrade pip to avoid metadata validation issues\n",
        "!pip install pip==23.2.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECETOFdVRYW-",
        "outputId": "554ab539-c3d9-4f5c-c98d-1351c638fb7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip==23.2.1\n",
            "  Downloading pip-23.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/2.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-23.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbxWVJfQtnq",
        "outputId": "0afde03f-155d-482c-8c76-80af7c0024c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from fairseq) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from fairseq) (3.0.12)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Obtaining dependency information for hydra-core<1.1,>=1.0.7 from https://files.pythonhosted.org/packages/5f/2a/9c698daa12ed6e09e7629e6908528f043fa9de8a441c56cc13608d765fb2/hydra_core-1.0.7-py3-none-any.whl.metadata\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq)\n",
            "  Obtaining dependency information for omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl.metadata\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Obtaining dependency information for sacrebleu>=1.4.12 from https://files.pythonhosted.org/packages/cd/45/7b55a7bd7e5c5b573b40ad58ba43fa09962dc5c8d71b1f573d4aeaa54a7e/sacrebleu-2.5.1-py3-none-any.whl.metadata\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from fairseq) (2.6.0+cu124)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Obtaining dependency information for bitarray from https://files.pythonhosted.org/packages/4e/80/47ccbcf0d78530ff5fce380d6689fa17def47bfac1e9753ae401503d75c5/bitarray-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading bitarray-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from fairseq) (2.6.0+cu124)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/f7/60/1974cfdd5bb770568ddc6f89f3e0df4cfdd1acffd5a609dff5e95f48c6e2/portalocker-3.1.1-py3-none-any.whl.metadata\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Obtaining dependency information for colorama from https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl.metadata\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq) (5.3.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->fairseq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->fairseq) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.4.127 from https://files.pythonhosted.org/packages/2c/14/91ae57cd4db3f9ef7aa99f4019cfa8d54cb4caa7e00975df6467e9725a9f/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.4.127 from https://files.pythonhosted.org/packages/ea/27/1795d86fe88ef397885f2e580ac37628ed058a92ed2c39dc8eac3adf0619/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.4.127 from https://files.pythonhosted.org/packages/67/42/f4f60238e8194a3106d06a058d494b18e006c10bb2b915655bd9f6ea4cb1/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-cudnn-cu12==9.1.0.70 from https://files.pythonhosted.org/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-cublas-cu12==12.4.5.8 from https://files.pythonhosted.org/packages/ae/71/1c91302526c45ab494c23f61c7a84aa568b8c1f9d196efa5993957faf906/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-cufft-cu12==11.2.1.3 from https://files.pythonhosted.org/packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-curand-cu12==10.3.5.147 from https://files.pythonhosted.org/packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-cusolver-cu12==11.6.1.9 from https://files.pythonhosted.org/packages/3a/e1/5b9089a4b2a4790dfdea8b3a006052cfecff58139d5a4e34cb1a51df8d6f/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-cusparse-cu12==12.3.1.170 from https://files.pythonhosted.org/packages/db/f7/97a9ea26ed4bbbfc2d470994b8b4f338ef663be97b8f677519ac195e113d/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->fairseq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->fairseq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->fairseq) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->fairseq)\n",
            "  Obtaining dependency information for nvidia-nvjitlink-cu12==12.4.127 from https://files.pythonhosted.org/packages/ff/ff/847841bacfbefc97a00036e0fce5a0f086b640756dc38caea5e1bb002655/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->fairseq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->fairseq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->fairseq) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->fairseq) (3.0.2)\n",
            "Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m654.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp311-cp311-linux_x86_64.whl size=11365871 sha256=6f931ba598bb7764d5b5fcbed1ca677f4255774475873df31fb5a64e0253151d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/0d/ed/f4ca5b65eef7dda06cb421355a049648c032c6322c455a396e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=521430b5534bbd62bd83f8b58ac81c1897c89d2588bb839e668ac263154fe5e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/10/be/9a70640a3a60ed4a7e1a45e49bb9f58b04692d5d7b517bd39e\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-3.2.0 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.0.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers fairseq nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch spacy requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N41uvJ7QbnmC",
        "outputId": "2ace56e5-9cef-4c95-e9f3-0bd41b2bf969"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mGENRE (Wikidata)"
      ],
      "metadata": {
        "id": "hBYmQyMorPhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This only finds entities"
      ],
      "metadata": {
        "id": "Vutcl3-ZtiT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import spacy\n",
        "\n",
        "# Load spaCy NER model for entity detection\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load GENRE model and tokenizer\n",
        "model_name = \"facebook/genre-linking-aidayago2\"  # You can use any available GENRE model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Full text to process\n",
        "text = \"\"\"Barack Obama, the 44th President of the United States, was born in Honolulu, Hawaii.\n",
        "He served two terms in office from 2009 to 2017.\n",
        "During his presidency, Obama signed the Affordable Care Act into law,\n",
        "a significant reform of the healthcare system. In 2015, Obama visited Cuba,\n",
        "marking the first time a sitting U.S. president had visited the island in nearly 90 years.\n",
        "After leaving office, he continued his work on global issues and humanitarian causes.\"\"\"\n",
        "\n",
        "# Process raw text with spaCy NER to get entities\n",
        "doc = nlp(text)\n",
        "entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "# Perform entity linking with GENRE for each detected entity\n",
        "for entity in entities:\n",
        "    # Link entity using GENRE (mGENRE will generate a valid entity name from input text)\n",
        "    inputs = tokenizer([f\"[START] {entity} [END]\"], return_tensors=\"pt\")  # You can skip [START]/[END] for GENRE\n",
        "    outputs = model.generate(**inputs)\n",
        "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Print the result\n",
        "    print(f\"Entity: {entity} → Linked to: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_-I5UC_rA0B",
        "outputId": "0cedc680-48ef-4265-e50a-87cee799f816"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Barack Obama → Linked to: Barack Obama\n",
            "Entity: 44th → Linked to: 44th United States Congress\n",
            "Entity: the United States → Linked to: United States\n",
            "Entity: Honolulu → Linked to: Honolulu\n",
            "Entity: Hawaii → Linked to: Hawaii\n",
            "Entity: two → Linked to: 2\n",
            "Entity: 2009 → Linked to: 2009 in paleontology\n",
            "Entity: Obama → Linked to: Barack Obama\n",
            "Entity: the Affordable Care Act → Linked to: Patient Protection and Affordable Care Act\n",
            "Entity: 2015 → Linked to: 2015 in paleontology\n",
            "Entity: Obama → Linked to: Barack Obama\n",
            "Entity: Cuba → Linked to: Cuba\n",
            "Entity: first → Linked to: First\n",
            "Entity: U.S. → Linked to: United States\n",
            "Entity: nearly 90 years → Linked to: Bertram Bingham III\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This code finds the Wikipedia URL."
      ],
      "metadata": {
        "id": "SEyLc8A7udx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import spacy\n",
        "\n",
        "# Load spaCy NER model for entity detection\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load GENRE model and tokenizer\n",
        "model_name = \"facebook/genre-linking-aidayago2\"  # You can use any available GENRE model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# # Full text to process\n",
        "# text = \"\"\"Barack Obama was born in Hawaii. He served as the 44th President of the United States. Hawaii is a state in the Pacific Ocean and in Iran. Hassan Rouhani was an Iranian president.\"\"\"\n",
        "# Full text to process\n",
        "text = \"\"\"Barack Obama, the 44th President of the United States, was born in Honolulu, Hawaii.\n",
        "He served two terms in office from 2009 to 2017.\n",
        "During his presidency, Obama signed the Affordable Care Act into law,\n",
        "a significant reform of the healthcare system. In 2015, Obama visited Cuba,\n",
        "marking the first time a sitting U.S. president had visited the island in nearly 90 years.\n",
        "After leaving office, he continued his work on global issues and humanitarian causes.\"\"\"\n",
        "\n",
        "# Process raw text with spaCy NER to get entities\n",
        "doc = nlp(text)\n",
        "entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "# Function to extract the Wikidata URL from the GENRE model output\n",
        "def get_wikidata_url(entity_name):\n",
        "    # Construct the Wikidata URL using the entity name (i.e., QID)\n",
        "    url = f\"https://en.wikipedia.org/wiki/{entity_name.replace(' ', '_')}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return url\n",
        "    else:\n",
        "        return f\"https://en.wikipedia.org/wiki/Q{entity_name}\"  # Fallback for QID\n",
        "\n",
        "\n",
        "# Process each sentence and link entities\n",
        "for sentence in entities:\n",
        "    # Link entity using GENRE\n",
        "    inputs = tokenizer([f\"[START] {sentence} [END]\"], return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    # print(result)\n",
        "    # Extract the Wikidata URL from the result\n",
        "    wikidata_url = get_wikidata_url(result)\n",
        "\n",
        "    if wikidata_url:\n",
        "        print(f\"Entity: {sentence} → Linked to: {wikidata_url}\")\n",
        "    else:\n",
        "        print(f\"Entity: {sentence} could not be linked to a valid Wikidata URL.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zTq6W0NrmSR",
        "outputId": "bcc0f112-90eb-4aec-f52b-2fa82f38cca3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Barack Obama → Linked to: https://en.wikipedia.org/wiki/Barack_Obama\n",
            "Entity: 44th → Linked to: https://en.wikipedia.org/wiki/44th_United_States_Congress\n",
            "Entity: the United States → Linked to: https://en.wikipedia.org/wiki/United_States\n",
            "Entity: Honolulu → Linked to: https://en.wikipedia.org/wiki/Honolulu\n",
            "Entity: Hawaii → Linked to: https://en.wikipedia.org/wiki/Hawaii\n",
            "Entity: two → Linked to: https://en.wikipedia.org/wiki/2\n",
            "Entity: 2009 → Linked to: https://en.wikipedia.org/wiki/2009_in_paleontology\n",
            "Entity: Obama → Linked to: https://en.wikipedia.org/wiki/Barack_Obama\n",
            "Entity: the Affordable Care Act → Linked to: https://en.wikipedia.org/wiki/Patient_Protection_and_Affordable_Care_Act\n",
            "Entity: 2015 → Linked to: https://en.wikipedia.org/wiki/2015_in_paleontology\n",
            "Entity: Obama → Linked to: https://en.wikipedia.org/wiki/Barack_Obama\n",
            "Entity: Cuba → Linked to: https://en.wikipedia.org/wiki/Cuba\n",
            "Entity: first → Linked to: https://en.wikipedia.org/wiki/First\n",
            "Entity: U.S. → Linked to: https://en.wikipedia.org/wiki/United_States\n",
            "Entity: nearly 90 years → Linked to: https://en.wikipedia.org/wiki/QBertram Bingham III\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linking to Wikidata (Spacy Only)"
      ],
      "metadata": {
        "id": "hMl5QlJJjh1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Sentence Matching"
      ],
      "metadata": {
        "id": "_TZvm_c4i9WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import spacy\n",
        "\n",
        "# Load spaCy NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load mGENRE model and tokenizer\n",
        "model_name = \"facebook/mgenre-wiki\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Raw text to process\n",
        "text = \"Barack Obama was born in Hawaii and served as the 44th President of the United States.\"\n",
        "\n",
        "# Detect entities using spaCy\n",
        "doc = nlp(text)\n",
        "entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "# Define a function to get QID from Wikidata for each entity\n",
        "def get_qid(entity_name):\n",
        "    url = f\"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"search\": entity_name,\n",
        "        \"language\": \"en\",\n",
        "        \"limit\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params).json()\n",
        "\n",
        "    if \"search\" in response and len(response[\"search\"]) > 0:\n",
        "        qid = response[\"search\"][0][\"id\"]\n",
        "        return f\"http://www.wikidata.org/entity/{qid}\"\n",
        "    else:\n",
        "        return None  # Return None if no matching entity is found\n",
        "\n",
        "# Link each entity to its Wikidata QID, removing any language tags\n",
        "for entity in entities:\n",
        "    # Strip language code (e.g., ' >> en') and use only the entity name\n",
        "    entity_name = entity.split(\" >>\")[0].strip()\n",
        "    if entity_name:  # Ensure that entity name is not empty\n",
        "        qid = get_qid(entity_name)\n",
        "        if qid:\n",
        "            print(f\"Entity: {entity_name} → Linked to QID: {qid}\")\n",
        "        else:\n",
        "            print(f\"Entity: {entity_name} could not be linked to a QID.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwEM2CdCbqri",
        "outputId": "e5026396-4621-432f-8d42-c6e2863c27c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Barack Obama → Linked to QID: http://www.wikidata.org/entity/Q76\n",
            "Entity: Hawaii → Linked to QID: http://www.wikidata.org/entity/Q782\n",
            "Entity: 44th → Linked to QID: http://www.wikidata.org/entity/Q21084461\n",
            "Entity: the United States → Linked to QID: http://www.wikidata.org/entity/Q30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Several Sentences Matching (Text)"
      ],
      "metadata": {
        "id": "Hnxuf6QyjBfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import requests\n",
        "\n",
        "# Load spaCy NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load mGENRE model and tokenizer\n",
        "model_name = \"facebook/mgenre-wiki\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Full text to process\n",
        "text = \"\"\"Barack Obama was born in Hawaii. He served as the 44th President of the United States. Hawaii is a state in the Pacific Ocean and in Iran. Hassan Rouhani was an Iranian president.\"\"\"\n",
        "\n",
        "# Function to get Wikidata QID\n",
        "def get_qid(entity_name):\n",
        "    url = f\"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"search\": entity_name,\n",
        "        \"language\": \"en\",\n",
        "        \"limit\": 1,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params).json()\n",
        "\n",
        "    if \"search\" in response and len(response[\"search\"]) > 0:\n",
        "        qid = response[\"search\"][0][\"id\"]\n",
        "        return f\"http://www.wikidata.org/entity/{qid}\"\n",
        "    else:\n",
        "        return None  # Return None if no matching entity is found\n",
        "\n",
        "# Split the text into sentences\n",
        "doc = nlp(text)\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "# Process each sentence\n",
        "for sentence in sentences:\n",
        "    print(f\"\\nProcessing sentence: {sentence}\")\n",
        "\n",
        "    # Detect entities using spaCy\n",
        "    doc = nlp(sentence)\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    # Link each entity to its Wikidata QID\n",
        "    for entity in entities:\n",
        "        # Strip language code (e.g., ' >> en') and use only the entity name\n",
        "        entity_name = entity.split(\" >>\")[0].strip()\n",
        "        if entity_name:  # Ensure that entity name is not empty\n",
        "            qid = get_qid(entity_name)\n",
        "            if qid:\n",
        "                print(f\"Entity: {entity_name} → Linked to QID: {qid}\")\n",
        "            else:\n",
        "                print(f\"Entity: {entity_name} could not be linked to a QID.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRfnsKOki1um",
        "outputId": "0351e0be-e7df-4bc2-b94f-77c52adc2e52"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing sentence: Barack Obama was born in Hawaii.\n",
            "Entity: Barack Obama → Linked to QID: http://www.wikidata.org/entity/Q76\n",
            "Entity: Hawaii → Linked to QID: http://www.wikidata.org/entity/Q782\n",
            "\n",
            "Processing sentence: He served as the 44th President of the United States.\n",
            "Entity: 44th → Linked to QID: http://www.wikidata.org/entity/Q21084461\n",
            "Entity: the United States → Linked to QID: http://www.wikidata.org/entity/Q30\n",
            "\n",
            "Processing sentence: Hawaii is a state in the Pacific Ocean and in Iran.\n",
            "Entity: Hawaii → Linked to QID: http://www.wikidata.org/entity/Q782\n",
            "Entity: the Pacific Ocean → Linked to QID: http://www.wikidata.org/entity/Q7755858\n",
            "Entity: Iran → Linked to QID: http://www.wikidata.org/entity/Q794\n",
            "\n",
            "Processing sentence: Hassan Rouhani was an Iranian president.\n",
            "Entity: Hassan Rouhani → Linked to QID: http://www.wikidata.org/entity/Q348144\n",
            "Entity: Iranian → Linked to QID: http://www.wikidata.org/entity/Q9168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import spacy\n",
        "import requests\n",
        "\n",
        "# Load spaCy NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load mGENRE model and tokenizer\n",
        "model_name = \"facebook/mgenre-wiki\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Full text to process\n",
        "text = \"\"\"Barack Obama was born in Hawaii. He served as the 44th President of the United States. Hawaii is a state in the Pacific Ocean and in Iran. Hassan Rouhani was an Iranian president.\"\"\"\n",
        "\n",
        "# Function to get QID from mGENRE model\n",
        "def link_entity_mgenre(entity_name):\n",
        "    # Use mGENRE to link the entity (replace with your mGENRE entity-linking logic)\n",
        "    inputs = tokenizer([f\"[START] {entity_name} [END]\"], return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    if \">>\" in result:\n",
        "        return f\"http://www.wikidata.org/entity/{result.split(' >> ')[1]}\"\n",
        "    return None\n",
        "\n",
        "# Split the text into sentences\n",
        "doc = nlp(text)\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "# Process each sentence\n",
        "for sentence in sentences:\n",
        "    print(f\"\\nProcessing sentence: {sentence}\")\n",
        "\n",
        "    # Detect entities using spaCy\n",
        "    doc = nlp(sentence)\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    # Link each entity to its Wikidata QID using mGENRE\n",
        "    for entity in entities:\n",
        "        # Link the entity to a QID using mGENRE\n",
        "        qid = link_entity_mgenre(entity)\n",
        "        if qid:\n",
        "            print(f\"Entity: {entity} → Linked to QID: {qid}\")\n",
        "        else:\n",
        "            print(f\"Entity: {entity} could not be linked to a QID.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfklQCuWpa3r",
        "outputId": "4eb19a6c-49ea-49e1-cf66-abcd1deb177c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing sentence: Barack Obama was born in Hawaii.\n",
            "Entity: Barack Obama → Linked to QID: http://www.wikidata.org/entity/en\n",
            "Entity: Hawaii → Linked to QID: http://www.wikidata.org/entity/it\n",
            "\n",
            "Processing sentence: He served as the 44th President of the United States.\n",
            "Entity: 44th → Linked to QID: http://www.wikidata.org/entity/en\n",
            "Entity: the United States → Linked to QID: http://www.wikidata.org/entity/en\n",
            "\n",
            "Processing sentence: Hawaii is a state in the Pacific Ocean and in Iran.\n",
            "Entity: Hawaii → Linked to QID: http://www.wikidata.org/entity/it\n",
            "Entity: the Pacific Ocean → Linked to QID: http://www.wikidata.org/entity/en\n",
            "Entity: Iran → Linked to QID: http://www.wikidata.org/entity/en\n",
            "\n",
            "Processing sentence: Hassan Rouhani was an Iranian president.\n",
            "Entity: Hassan Rouhani → Linked to QID: http://www.wikidata.org/entity/en\n",
            "Entity: Iranian → Linked to QID: http://www.wikidata.org/entity/en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia"
      ],
      "metadata": {
        "id": "Sh_SmLI0mdNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import spacy\n",
        "import requests\n",
        "\n",
        "# Load spaCy NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load mGENRE model and tokenizer\n",
        "model_name = \"facebook/mgenre-wiki\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Raw text to process\n",
        "text = \"\"\"Barack Obama was born in Hawaii. He served as the 44th President of the United States. Hawaii is a state in the Pacific Ocean and in Iran. Hassan Rouhani was an Iranian president.\"\"\"\n",
        "\n",
        "# Detect entities using spaCy\n",
        "doc = nlp(text)\n",
        "entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "# Define a function to get Wikipedia URL from entity name or QID\n",
        "def get_wikipedia_url(entity_name):\n",
        "    # First check if the entity is in Wikipedia\n",
        "    url = f\"https://en.wikipedia.org/wiki/{entity_name.replace(' ', '_')}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return url\n",
        "    else:\n",
        "        return f\"https://en.wikipedia.org/wiki/Q{entity_name}\"  # Fallback for QID\n",
        "\n",
        "# Process each entity with mGENRE to link to Wikidata QID and Wikipedia URL\n",
        "for entity in entities:\n",
        "    # Link entity using mGENRE\n",
        "    inputs = tokenizer([f\"[START] {entity} [END]\"], return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Ensure the output is a valid Wikidata entity\n",
        "    if \">>\" in result:\n",
        "        entity_name = result.split(\" >>\")[0].strip()\n",
        "        wikipedia_url = get_wikipedia_url(entity_name)\n",
        "        print(f\"Entity: {entity_name} → Wikipedia URL: {wikipedia_url}\")\n",
        "    else:\n",
        "        print(f\"Entity: {entity} could not be linked to a Wikipedia URL.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hhw4JNPll7L",
        "outputId": "89de0fcd-cd59-4fd1-ce69-4d08c4605624"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Barack Obama → Wikipedia URL: https://en.wikipedia.org/wiki/Barack_Obama\n",
            "Entity: Hawaii → Wikipedia URL: https://en.wikipedia.org/wiki/Hawaii\n",
            "Entity: 44th Division → Wikipedia URL: https://en.wikipedia.org/wiki/44th_Division\n",
            "Entity: United States → Wikipedia URL: https://en.wikipedia.org/wiki/United_States\n",
            "Entity: Hawaii → Wikipedia URL: https://en.wikipedia.org/wiki/Hawaii\n",
            "Entity: Pacific Ocean → Wikipedia URL: https://en.wikipedia.org/wiki/Pacific_Ocean\n",
            "Entity: Iran → Wikipedia URL: https://en.wikipedia.org/wiki/Iran\n",
            "Entity: Hassan Rouhani → Wikipedia URL: https://en.wikipedia.org/wiki/Hassan_Rouhani\n",
            "Entity: Iranian peoples → Wikipedia URL: https://en.wikipedia.org/wiki/Iranian_peoples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linking to DBPedia"
      ],
      "metadata": {
        "id": "L4dgnu4MjoJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMrn7ifqkVwW",
        "outputId": "685caf1e-bb8a-4338-f7f6-2f600ec7861d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Obtaining dependency information for fuzzywuzzy from https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import requests\n",
        "\n",
        "# Load spaCy NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to get DBpedia URI for an entity\n",
        "def get_dbpedia_uri(entity_name):\n",
        "    # Define the SPARQL endpoint for DBpedia\n",
        "    endpoint = \"https://dbpedia.org/sparql\"\n",
        "\n",
        "    # Define the SPARQL query to search for the entity in DBpedia\n",
        "    query = f\"\"\"\n",
        "    SELECT ?resource\n",
        "    WHERE {{\n",
        "        ?resource rdfs:label \"{entity_name}\"@en.\n",
        "    }}\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        "\n",
        "    # Send the request to the SPARQL endpoint\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(endpoint, params=params)\n",
        "\n",
        "    # Parse the response and return the URI if found\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data[\"results\"][\"bindings\"]:\n",
        "            # Extract the URI from the response\n",
        "            dbpedia_uri = data[\"results\"][\"bindings\"][0][\"resource\"][\"value\"]\n",
        "            return dbpedia_uri\n",
        "    return None\n",
        "\n",
        "# Full text to process\n",
        "text = \"\"\"Barack Obama was born in Hawaii. He served as the 44th President of the United States. Hawaii is a state in the Pacific Ocean and Iran.\"\"\"\n",
        "\n",
        "# Split the text into sentences\n",
        "doc = nlp(text)\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "# Process each sentence\n",
        "for sentence in sentences:\n",
        "    print(f\"\\nProcessing sentence: {sentence}\")\n",
        "\n",
        "    # Detect entities using spaCy\n",
        "    doc = nlp(sentence)\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    # Link each entity to its DBpedia URI\n",
        "    for entity in entities:\n",
        "        # Strip any unwanted characters and use only the entity name\n",
        "        entity_name = entity.strip()\n",
        "        if entity_name:  # Ensure that entity name is not empty\n",
        "            dbpedia_uri = get_dbpedia_uri(entity_name)\n",
        "            if dbpedia_uri:\n",
        "                print(f\"Entity: {entity_name} → Linked to DBpedia URI: {dbpedia_uri}\")\n",
        "            else:\n",
        "                print(f\"Entity: {entity_name} could not be linked to a DBpedia URI.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsl8zKCNjryk",
        "outputId": "52292b54-342a-456a-8610-f92e28336ea8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing sentence: Barack Obama was born in Hawaii.\n",
            "Entity: Barack Obama → Linked to DBpedia URI: http://dbpedia.org/resource/Barack_Obama\n",
            "Entity: Hawaii → Linked to DBpedia URI: http://dbpedia.org/resource/Category:Hawaii\n",
            "\n",
            "Processing sentence: He served as the 44th President of the United States.\n",
            "Entity: 44th could not be linked to a DBpedia URI.\n",
            "Entity: the United States could not be linked to a DBpedia URI.\n",
            "\n",
            "Processing sentence: Hawaii is a state in the Pacific Ocean and Iran.\n",
            "Entity: Hawaii → Linked to DBpedia URI: http://dbpedia.org/resource/Category:Hawaii\n",
            "Entity: the Pacific Ocean could not be linked to a DBpedia URI.\n",
            "Entity: Iran → Linked to DBpedia URI: http://dbpedia.org/resource/Category:Iran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The rest"
      ],
      "metadata": {
        "id": "ResPdACIisq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"facebook/mgenre-wiki\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, from_tf=True)\n",
        "\n",
        "# Sample text (with multiple entities)\n",
        "text = \"[START] Barack Obama [END] was born in [START] Hawaii [END].\"\n",
        "\n",
        "# Split the text into individual entity mentions\n",
        "mentions = [\"[START] Barack Obama [END]\", \"[START] Hawaii [END]\"]\n",
        "\n",
        "# Process each mention\n",
        "for mention in mentions:\n",
        "    # Tokenize and run model\n",
        "    inputs = tokenizer([mention], return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "\n",
        "    # Decode and print result\n",
        "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    print(f\"{mention} → {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfsHOBFwSVq2",
        "outputId": "bc8789b3-3fda-42a6-f31b-503ed2c0c8e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing MBartForConditionalGeneration.\n",
            "\n",
            "Some weights of MBartForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START] Barack Obama [END] → ['Barack Obama >> en']\n",
            "[START] Hawaii [END] → ['Hawaii >> it']\n"
          ]
        }
      ]
    }
  ]
}